import numpy as np  # Импорт библиотеки NumPy для работы с массивами и матрицами
import matplotlib.pyplot as plt  # Импорт библиотеки Matplotlib для визуализации данных
from sklearn.metrics import r2_score  # Импорт функции r2_score из библиотеки scikit-learn для расчета коэффициента детерминации
from sklearn.linear_model import LinearRegression  # Импорт класса LinearRegression из scikit-learn для построения линейной регрессии
from scipy.stats import f  # Импорт функции f из библиотеки SciPy для работы с F-распределением

# Данные из таблицы
x_data = np.array([0, 3, 5, 10, 15, 17, 20, 22, 25])  # Массив значений переменной x
y_data = np.array([7.2, 7.05 ,6.95, 6.7, 6.65, 6.61, 6.55, 6.50, 6.45])  # Массив значений переменной y

# Аппроксимация многочленом первой степени
p1, residuals_1, _, _, _ = np.polyfit(x_data, y_data, 1, full=True)  # Построение полинома первой степени и расчет остатков
print("Коэффициенты многочлена первой степени:", p1)  # Вывод коэффициентов полинома первой степени

# Аппроксимация многочленом второй степени
p2, residuals_2, _, _, _ = np.polyfit(x_data, y_data, 2, full=True)  # Построение полинома второй степени и расчет остатков
print("Коэффициенты многочлена второй степени:", p2)  # Вывод коэффициентов полинома второй степени

# Экспоненциальная аппроксимация с исправлением
y_data_log = np.log(y_data)  # Вычисление натурального логарифма для значений y
p_log, residuals_log, _, _, _ = np.polyfit(x_data, y_data_log, 1, full=True)  # Построение линейной модели для логарифмированных данных
a1_exp = np.exp(p_log[1])  # Получение параметра a1 для экспоненциальной модели
a2_exp = p_log[0]  # Получение параметра a2 для экспоненциальной модели
print("Параметры экспоненциальной модели:", (a1_exp, a2_exp))  # Вывод параметров экспоненциальной модели

# Расчет предсказанных значений
y_pred_1 = np.polyval(p1, x_data)  # Рассчитываем предсказанные значения для линейной модели
y_pred_2 = np.polyval(p2, x_data)  # Рассчитываем предсказанные значения для квадратичной модели
y_pred_exp = a1_exp * np.exp(a2_exp * x_data)  # Рассчитываем предсказанные значения для экспоненциальной модели

# Расчет коэффициента детерминированности R^2
r2_1 = r2_score(y_data, y_pred_1)  # Рассчитываем коэффициент детерминации для линейной модели
r2_2 = r2_score(y_data, y_pred_2)  # Рассчитываем коэффициент детерминации для квадратичной модели
r2_exp = r2_score(y_data, y_pred_exp)  # Рассчитываем коэффициент детерминации для экспоненциальной модели

# Расчет остаточной дисперсии
residual_variance_1 = residuals_1 / (len(y_data) - 2)  # Рассчитываем остаточную дисперсию для линейной модели
residual_variance_2 = residuals_2 / (len(y_data) - 3)  # Рассчитываем остаточную дисперсию для квадратичной модели
residual_variance_exp = residuals_log / (len(y_data) - 2)  # Рассчитываем остаточную дисперсию для экспоненциальной модели

print("R^2 и остаточная дисперсия для линейной модели:", (r2_1, residual_variance_1[0]))  # Выводим коэффициент детерминации и остаточную дисперсию для линейной модели
print("R^2 и остаточная дисперсия для квадратичной модели:", (r2_2, residual_variance_2[0]))  # Выводим коэффициент детерминации и остаточную дисперсию для квадратичной модели
print("R^2 и остаточная дисперсия для экспоненциальной модели:", (r2_exp, residual_variance_exp[0]))  # Выводим коэффициент детерминации и остаточную дисперсию для экспоненциальной модели

# Расчет коэффициента корреляции
correlation_coefficient = np.corrcoef(x_data, y_data)[0, 1]  # Рассчитываем коэффициент корреляции между x и y
print("Коэффициент корреляции:", correlation_coefficient)  # Выводим коэффициент корреляции

# Создаем график с исходными данными
plt.figure(figsize=(10, 6))  # Создаем новый график с размерами 10x6 дюймов
plt.scatter(x_data, y_data, color='black', label='Исходные данные')  # Рисуем точечный график исходных данных

# Добавляем линии тренда для каждой модели
x_model = np.linspace(x_data.min(), x_data.max(), 100)  # Генерируем 100 равномерно распределенных значений между минимальным и максимальным значениями x
y_model_1 = np.polyval(p1, x_model)  # Рассчитываем значения линейной модели для этих значений x
y_model_2 = np.polyval(p2, x_model)  # Рассчитываем значения квадратичной модели для этих значений x
y_model_exp = a1_exp * np.exp(a2_exp * x_model)  # Рассчитываем значения экспоненциальной модели для этих значений x

plt.plot(x_model, y_model_1, label='Линейная модель', color='blue')  # Добавляем линию линейной модели на график
plt.plot(x_model, y_model_2, label='Квадратичная модель', color='green')  # Добавляем линию квадратичной модели на график
plt.plot(x_model, y_model_exp, label='Экспоненциальная модель', color='red')  # Добавляем линию экспоненциальной модели на график

plt.title('Линии тренда для разных моделей аппроксимации')  # Устанавливаем заголовок графика
plt.xlabel('Содержание наполнителей')  # Устанавливаем подпись оси x
plt.ylabel('Скорость детонации')  # Устанавливаем подпись оси y
plt.legend()  # Добавляем легенду
plt.grid(True)  # Включаем сетку
plt.show()  # Показываем график

# Создаем объект линейной регрессии
lin_reg = LinearRegression()  # Создаем экземпляр класса LinearRegression

# Подгоняем модель
lin_reg.fit(x_data.reshape(-1, 1), y_data)  # Обучаем модель на данных

# Коэффициенты модели
coef_lin_reg = (lin_reg.intercept_, lin_reg.coef_[0])  # Получаем коэффициенты линейной регрессии
print("Коэффициенты линейной регрессии:", coef_lin_reg)  # Выводим коэффициенты линейной регрессии

# Определение количества степеней свободы
df_total = len(x_data) - 1  # Общее количество степеней свободы
df_model_1 = 1  # Количество степеней свободы модели для линейной модели
df_model_2 = 2  # Количество степеней свободы модели для квадратичной модели
df_error_1 = df_total - df_model_1  # Остаточные степени свободы для линейной модели
df_error_2 = df_total - df_model_2  # Остаточные степени свободы для квадратичной модели
df_error_exp = df_total - df_model_1  # Остаточные степени свободы для экспоненциальной модели (линейное приближение)

# Расчет сумм квадратов отклонений
SS_total = np.sum((y_data - np.mean(y_data))**2)  # Общая сумма квадратов отклонений
SS_res_1 = np.sum((y_data - y_pred_1)**2)  # Сумма квадратов остатков для линейной модели
SS_res_2 = np.sum((y_data - y_pred_2)**2)  # Сумма квадратов остатков для квадратичной модели
SS_res_exp = np.sum((y_data - y_pred_exp)**2)  # Сумма квадратов остатков для экспоненциальной модели

# Расчет сумм квадратов регрессии
SS_reg_1 = SS_total - SS_res_1  # Сумма квадратов регрессии для линейной модели
SS_reg_2 = SS_total - SS_res_2  # Сумма квадратов регрессии для квадратичной модели
SS_reg_exp = SS_total - SS_res_exp  # Сумма квадратов регрессии для экспоненциальной модели

# Расчет F-статистики
F_1 = (SS_reg_1 / df_model_1) / (SS_res_1 / df_error_1)  # F-статистика для линейной модели
F_2 = (SS_reg_2 / df_model_2) / (SS_res_2 / df_error_2)  # F-статистика для квадратичной модели
F_exp = (SS_reg_exp / df_model_1) / (SS_res_exp / df_error_exp)  # F-статистика для экспоненциальной модели

# Расчет p-значений для F-статистики
p_value_1 = 1 - f.cdf(F_1, df_model_1, df_error_1)  # p-значение для F-статистики для линейной модели
p_value_2 = 1 - f.cdf(F_2, df_model_2, df_error_2)  # p-значение для F-статистики для квадратичной модели
p_value_exp = 1 - f.cdf(F_exp, df_model_1, df_error_exp)  # p-значение для F-статистики для экспоненциальной модели
print("F-статистика и p-значение для линейной модели:", (F_1, p_value_1))  # Вывод F-статистики и p-значения для линейной модели
print("F-статистика и p-значение для квадратичной модели:", (F_2, p_value_2))  # Вывод F-статистики и p-значения для квадратичной модели
print("F-статистика и p-значение для экспоненциальной модели:", (F_exp, p_value_exp))  # Вывод F-статистики и p-значения для экспоненциальной модели

# Матрицы дизайна для линейной и квадратичной модели
X_1 = np.vstack([np.ones(len(x_data)), x_data]).T  # Матрица дизайна для линейной модели
X_2 = np.vstack([np.ones(len(x_data)), x_data, x_data**2]).T  # Матрица дизайна для квадратичной модели

# Оценки дисперсии на основе остатков
variance_est_1 = residuals_1 / df_error_1  # Оценка дисперсии для линейной модели
variance_est_2 = residuals_2 / df_error_2  # Оценка дисперсии для квадратичной модели

# Расчет ковариационных матриц для линейной и квадратичной модели
cov_matrix_1 = variance_est_1 * np.linalg.inv(X_1.T.dot(X_1))  # Ковариационная матрица для линейной модели
cov_matrix_2 = variance_est_2 * np.linalg.inv(X_2.T.dot(X_2))  # Ковариационная матрица для квадратичной модели

# Расчет стандартной ошибки коэффициентов и t-статистики для обеих моделей
std_err_1 = np.sqrt(np.diag(cov_matrix_1))  # Стандартная ошибка коэффициентов для линейной модели
t_stats_1 = p1 / std_err_1  # t-статистика для линейной модели

std_err_2 = np.sqrt(np.diag(cov_matrix_2))  # Стандартная ошибка коэффициентов для квадратичной модели
t_stats_2 = p2 / std_err_2  # t-статистика для квадратичной модели

print("Стандартные ошибки и t-статистики для линейной модели:", (std_err_1, t_stats_1))  # Вывод стандартных ошибок и t-статистик для линейной модели
print("Стандартные ошибки и t-статистики для квадратичной модели:", (std_err_2, t_stats_2))  # Вывод стандартных ошибок и t-статистик для квадратичной модели
